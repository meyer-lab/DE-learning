## Methods

### PageRank node characterization




### Perturbation model

The model was constructed using Julia 1.5.0. The levels of mRNA in response to different perturbations are described by a unified system of ordinary differential equations. In this equation, $x_{i}^\mu$ represents the expression level of each gene under a specific knockout condition $\mu$. $\omega$ is a matrix representing interactions between different genes, specifically, $\omega_{ij}$ indicate interaction between gene j on gene i. To simulate the knockout condition, the corresponding column in $\omega$ is set to 0. All the elements in $\omega$ are also assumed to be constant after the introduction of a specific perturbation. We use function $\mathit{1 + tanh( )}$ to model the saturation effect of the interaction term and avoid negative values. Then $\epsilon$ is used as a saturation coefficient to bound the whole interaction term. Finally, we use $\alpha$ to characterize the degradation rate of each mRNA. The initial values of state variables x are approximated by $\mathit{\epsilon / \alpha}$, derived from steady-state assumption of the ODE equation while ignoring the effect of the $\omega$ matrix. The ODE system was numerically solved using the AutoVern7 method with low tolerance. In addition, the Jacobian matrix of the ODE equation was symbolically calculated and provided to the ODE solver for performance. The model performance was evaluated by difference between RNA-Seq Data and simulation output (the comparison was also visualized by using scatterplot of Model outputs vs RNA-Seq Measurements).

### Fused perturbation solving

By assuming that every treatment should be at steady-state, the fitting process can do away with the ODE solver. In other words, after initialization, the parameters can be optimized by minimizing a cost function that calculate the norm of deviation from steady-state values, ensuring the output of model is consistent with experiment data. And to improve the performance of optimization, all the knockout conditions are solved with matrix math:

$$cost = norm(\epsilon \cdot (1 + tanh(\omega\cdot U)) - \alpha\cdot D)$$

D is an 83 x 84 experiment data matrix while U is a copy of D. Instead of modifying w matrix, the diagonal elements in U matrix are set to zero in order to simulate each knockout condition. As a result, this allows to solve the model in a single pass:

$$U[diagind(U)] =0$$


### Iterative matrix solving method

As described above, all the perturbations can be described at steady state by solving the matrix equation:

$$\bar{\epsilon} \left( 1 + \tanh⁡\left(W U \right) \right) = \alpha X$$

Given steady-state measurements, $\alpha$ cannot be identified separately from $\bar{\epsilon}$ and so we set the value of the former to 0.1. This leaves two co-dependent unknowns $\bar{\epsilon}$ and W to be determined. However, given some specified value of W, $\bar{\epsilon}$ can be found using the geometric mean of each column from the ratio of $\alpha X$ and $1 + \tanh(WU)$. We additionally limited the minimum value of both expressions to be 0.1, to avoid issues arising from genes with no expression. Conversely, with fixed $\bar{\epsilon}$ one can solve for W using the expression:

$$B = \tanh^{-1}⁡(αX/\bar{\epsilon} - 1)$$

$$W = ( {U'}^{*}  B' )'$$

Where ${U'}^{*}$ indicates the pseudoinverse of $U’$. The values were restricted to the domain -0.9 to 0.9 before atanh transformation to reduce the effect of outlier values. Consequently, one can iteratively solve for the overall solution by repeated and alternating solving for each variable. We found this to generally converge within 10 iterations.

Conveniently, this approach can also account for various regularization for the structure of $W$, including orthogonality and sparsity, through generalizations of the pseudoinverse [@DOI:10.1137/15M1028479; @DOI:10.1137/0119015]. In any practical situation the pseudoinverse is also the predominant cost for solving and is sure to be far less costly than generic optimization with or without solving the ordinary differential equations.


### gradient of cost w.r.t. w

$$\frac{d cost}{dw} = \frac{d}{dw} Tr\Big[((\eta \circ E(w))^T - \alpha D) ((\eta \circ E(w))^T - \alpha D)^T \Big]$$

where $E(w) = \frac{1}{1 + e^{-wU}}$ and $E(w)^T = \frac{1}{1 + e^{-U^Tw^T}}$.

$$= \frac{d}{dw} Tr\Big[(\eta \circ E(w))^T (\eta \circ E(w)) \Big] - 2 \alpha Tr\Big[D (\eta \circ E(w)) \Big]$$
The two parts in the above expression could be summarized as:

$$\frac{d}{dw} Tr\Big[(\eta \circ E(w))^T (\eta \circ E(w)) \Big] = Tr\Big[ \frac{d}{dw} (\eta^T \circ E(w)^T)(\eta \circ E(w)) + (\eta^T \circ E(w)^T) \frac{d}{dw}(\eta \circ E(w))\Big]$$
$$= Tr(\eta^T \circ \frac{d}{dw} E(w)^T (\eta \circ E(w))) + Tr((\eta^T \circ E(w)^T)(\eta \circ \frac{d}{dw} E(w)))$$
and
$$ - 2 \alpha \frac{d}{dw} Tr\Big[D (\eta \circ E(w))\Big] = -2 \alpha Tr(D \frac{d}{dw} (\eta \circ E(w))) = -2 \alpha Tr\Big(D (\eta \circ \frac{d}{dw} E(w)) \Big)$$
Besides, we have:
$$\frac{d}{dw} E(w) = \frac{-U}{(1 + e^{-wU})^2} = -U(E(w) (1 - E(w)))$$


